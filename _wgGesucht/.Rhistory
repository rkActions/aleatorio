}) %>% unlist
links_locations_2
# select 10 random links
sample_locations = sample(links_locations_2, 5)
sample_locations
# read the html from these locations
links = map(sample_locations, function(u){
base = "https://www.wg-gesucht.de/"
url = glue("{base}{u}")
raw_html = read_html(url)
links = raw_html %>% html_elements("h3 > a") %>% lapply(function(x) {
href=html_attr(x, "href")
link=glue("{base}{href}")
}) %>% unlist
return(links)
}) %>% unlist
links
links_locations_1 = html_attr(a_tags_share_flats, "href")
links_locations_2 = map(links_locations_1, function(l){
if(str_detect(l, "1-zimmer")){
l = str_replace(l, "1-zimmer-wohnungen", "wg-zimmer")
}
return(l)
}) %>% unlist
# select 10 random links
sample_locations = sample(links_locations_2, 5)
# read the html from these locations
links = map(sample_locations, function(u){
base = "https://www.wg-gesucht.de/"
url = glue("{base}{u}")
raw_html = read_html(url)
links = raw_html %>% html_elements("h3 > a") %>% lapply(function(x) {
href=html_attr(x, "href")
link=glue("{base}{href}")
}) %>% unlist
return(links)
}) %>% unlist
links
links_locations_1
links_locations_2
# select 10 random links
sample_locations = sample(links_locations_2, 5)
sample_locations
# select 10 random links
sample_locations = sample(links_locations_2, 5)
sample_locations
# select 10 random links
sample_locations = sample(links_locations_2, 5)
sample_locations
u = sample_locations[[1]]
u
base = "https://www.wg-gesucht.de/"
url = glue("{base}{u}")
raw_html = read_html(url)
links = raw_html %>% html_elements("h3 > a") %>% lapply(function(x) {
href=html_attr(x, "href")
link=glue("{base}{href}")
}) %>% unlist
links
raw_html
url
l
l = str_replace(l, "\\.1\\.1\\.0", ".0.1.0")
l
links_locations_2 = map(links_locations_1, function(l){
if(str_detect(l, "1-zimmer")){
l = str_replace(l, "1-zimmer-wohnungen", "wg-zimmer")
l = str_replace(l, "\\.1\\.1\\.0", ".0.1.0")
}
return(l)
}) %>% unlist
links_locations_2
# select 10 random links
sample_locations = sample(links_locations_2, 5)
# read the html from these locations
links = map(sample_locations, function(u){
base = "https://www.wg-gesucht.de/"
url = glue("{base}{u}")
raw_html = read_html(url)
links = raw_html %>% html_elements("h3 > a") %>% lapply(function(x) {
href=html_attr(x, "href")
link=glue("{base}{href}")
}) %>% unlist
return(links)
}) %>% unlist
links
sample_locations
# select 10 random links
sample_locations = sample(links_locations_2, 5)
sample_locations
# select 10 random links
sample_locations = sample(links_locations_2, 5)
sample_locations
# select 10 random links
sample_locations = sample(links_locations_2, 5)
sample_locations
u = sample_locations[[2]]
u
base = "https://www.wg-gesucht.de/"
url = glue("{base}{u}")
raw_html = read_html(url)
url
links = raw_html %>% html_elements("h3 > a") %>% lapply(function(x) {
href=html_attr(x, "href")
link=glue("{base}{href}")
}) %>% unlist
return(links)
links = raw_html %>% html_elements("h3 > a") %>% lapply(function(x) {
href=html_attr(x, "href")
link=glue("{base}{href}")
}) %>% unlist
links
raw_html %>% html_elements("h3 > a")
raw_html
url
raw_html = read_html(url)
raw_html
url
url
raw_html = read_html(url)
links = raw_html %>% html_elements("h3 > a") %>% lapply(function(x){
html_attr(x, "href")
}) %>% unlist
links
url="https://www.wg-gesucht.de/wg-zimmer-in-Berlin.8.0.1.0.html"
raw_html = read_html(url)
links = raw_html %>% html_elements("h3 > a") %>% lapply(function(x){
html_attr(x, "href")
}) %>% unlist
links
download.file(url, "~/Desktop/test.html")
url
u
base = "https://www.wg-gesucht.de/"
url = glue("{base}{u}")
raw_html = read_html(url)
links = raw_html %>% html_elements("h3 > a") %>% lapply(function(x) {
href=html_attr(x, "href")
link=glue("{base}{href}")
}) %>% unlist
links
source(here("analysis/utils.R"))
# get the urls ------------------------------------------------------------
urls = getUrls()
df[["date"]] = as.Date(df[["date"]])
urls
# date --------------------------------------------------------------------
today = Sys.Date()
rows = vector("list", length=length(urls))
i = 1
print(i)
# id = page_ids[[i]]
# url = paste0("https://www.wg-gesucht.de/", id, ".html" )
url = urls[[i]]
file = paste0(tempfile(), ".html")
# download
download.file(url, file)
# read
html = rvest::read_html(file)
size = getSize(html) %>% as.numeric()
price = getPrice(html) %>% as.numeric()
ab_bis = freiAb(html)
ab = ab_bis$ab
bis = ab_bis$bis
location = getLocation(html) %>% as.character()
wg = getWG(file)
wg
rows = vector("list", length=length(urls))
for(i in seq_along(urls)){
print(i)
# id = page_ids[[i]]
# url = paste0("https://www.wg-gesucht.de/", id, ".html" )
url = urls[[i]]
file = paste0(tempfile(), ".html")
# download
download.file(url, file)
# read
html = rvest::read_html(file)
size = getSize(html) %>% as.numeric()
price = getPrice(html) %>% as.numeric()
ab_bis = freiAb(html)
ab = ab_bis$ab
bis = ab_bis$bis
location = getLocation(html) %>% as.character()
wg = getWG(file)
print(paste0("WG: ", wg))
row = list(price = price, size = size, location = location, id = id, wg=wg, date=today, ab=ab, bis=bis)
rows[[i]] = row
}
links = links[1:60]
links
# get the urls ------------------------------------------------------------
urls = getUrls()
urls
# get the urls ------------------------------------------------------------
urls = getUrls()
urls
url_all_cities = "https://www.wg-gesucht.de/wohngemeinschaft.html"
html_all_cities = read_html(url_all_cities)
# get all the urls to the shared flats
a_tags = html_all_cities %>% html_elements("a")
a_tags_share_flats = a_tags[str_detect(tolower(html_text(a_tags, trim = T)), "share")]
links_locations_1 = html_attr(a_tags_share_flats, "href")
links_locations_2 = lapply(links_locations_1, function(l){
if(str_detect(l, "1-zimmer")){
l = str_replace(l, "1-zimmer-wohnungen", "wg-zimmer")
l = str_replace(l, "\\.1\\.1\\.0", ".0.1.0")
}
return(l)
}) %>% unlist
# select 10 random links
sample_locations = sample(links_locations_2, 5)
sample_locations
# read the html from these locations
links = lapply(sample_locations, function(u){
base = "https://www.wg-gesucht.de/"
url = glue("{base}{u}")
raw_html = read_html(url)
links = raw_html %>% html_elements("h3 > a") %>% lapply(function(x) {
href=html_attr(x, "href")
link=glue("{base}{href}")
}) %>% unlist
return(links)
}) %>% unlist
links
base = "https://www.wg-gesucht.de/"
url = glue("{base}{u}")
url
download.file(url, "~/Desktop/test.html")
raw_html = read_html(url)
links = raw_html %>% html_elements("h3 > a") %>% lapply(function(x) {
href=html_attr(x, "href")
link=glue("{base}{href}")
}) %>% unlist
links
base = "https://www.wg-gesucht.de/"
url = glue("{base}{u}")
raw_html = read_html(url)
raw_html
write(raw_html, "~/Desktop/raw.html")
cat(raw_html, "~/Desktop/raw.html")
cat(raw_html[2], "~/Desktop/raw.html")
raw_html[2]
raw_html[[2]]
url="https://www.wg-gesucht.de/wg-zimmer-in-Berlin.8.0.1.0.html"
raw_html = read_html(url)
links = raw_html %>% html_elements("h3 > a") %>% lapply(function(x){
html_attr(x, "href")
}) %>% unlist
links
url = "https://www.wg-gesucht.de//wg-zimmer-in-Moskau-Sawjolowskij-Rajon-Dynamo-Sawjolowskaja.5126338.html"
file = paste0(tempfile(), ".html")
# download
download.file(url, file)
# read
html = rvest::read_html(file)
size = getSize(html) %>% as.numeric()
size
html
file
# download
download.file(url, file)
url = "https://www.wg-gesucht.de//wg-zimmer-in-Wuppertal.142.0.1.0.html?asset_id=10622930&pu=12685206&sort_column=1&sort_order=0"
# id = page_ids[[i]]
# url = paste0("https://www.wg-gesucht.de/", id, ".html" )
url = urls[[i]]
file = paste0(tempfile(), ".html")
# download
download.file(url, file)
# read
html = rvest::read_html(file)
url
url="https://www.wg-gesucht.de//wg-zimmer-in-Wuppertal.142.0.1.0.html?asset_id=10622930&pu=12685206&sort_column=1&sort_order=0"
url
# download
download.file(url, file)
# read
html = rvest::read_html(file)
size = getSize(html) %>% as.numeric()
size
price = getPrice(html) %>% as.numeric()
ab_bis = freiAb(html)
ab_bis
html
sections = html %>%
html_elements(".col-xs-12.col-sm-6")
sections_inner = lapply(sections, function(x){
t = x %>%
html_elements("h3") %>%
html_text(trim = T)
if(length(t) > 0 && t == "VerfÃ¼gbarkeit"){
return(x)
}
return(NA)
})
if (length(sections_inner) == 0) {
return(list(ab = NA_character_, bis = NA_character_))
}
sections_inner
section_verf = sections_inner[!is.na(sections_inner)][[1]]
!is.na(sections_inner)
is.na(sections_inner)
sections_inner
section_verf = tryCatch({
section_verf = sections_inner[!is.na(sections_inner)][[1]]
}, error = function() {
return(NA)
})
section_verf = tryCatch({
section_verf = sections_inner[!is.na(sections_inner)][[1]]
}, error = function(cond) {
return(NA)
})
section_verf
url="https://www.wg-gesucht.de/cuba.html?page=//wg-zimmer-in-Erding-Erding.8604186.html"
# id = page_ids[[i]]
# url = paste0("https://www.wg-gesucht.de/", id, ".html" )
url = urls[[i]]
file = paste0(tempfile(), ".html")
# download
download.file(url, file)
url
url="https://www.wg-gesucht.de/cuba.html?page=//wg-zimmer-in-Erding-Erding.8604186.html"
# id = page_ids[[i]]
# url = paste0("https://www.wg-gesucht.de/", id, ".html" )
url = urls[[i]]
file = paste0(tempfile(), ".html")
# download
download.file(url, file)
url
url="https://www.wg-gesucht.de/cuba.html?page=//wg-zimmer-in-Erding-Erding.8604186.html"
# download
download.file(url, file)
# read
html = rvest::read_html(file)
# get the urls ------------------------------------------------------------
urls = getUrls()
df[["date"]] = as.Date(df[["date"]])
df[["id"]] = as.character(df[["id"]])
# date --------------------------------------------------------------------
today = Sys.Date()
rows = vector("list", length=length(urls))
urls
for(i in seq_along(urls)){
print(i)
# id = page_ids[[i]]
# url = paste0("https://www.wg-gesucht.de/", id, ".html" )
url = urls[[i]]
file = paste0(tempfile(), ".html")
# download
download.file(url, file)
# read
html = rvest::read_html(file)
size = getSize(html) %>% as.numeric()
price = getPrice(html) %>% as.numeric()
ab_bis = freiAb(html)
ab = ab_bis$ab
bis = ab_bis$bis
location = getLocation(html) %>% as.character()
wg = getWG(file)
print(paste0("WG: ", wg))
row = list(price = price, size = size, location = location, id = url, wg=wg, date=today, ab=ab, bis=bis)
rows[[i]] = row
}
urls
i=1
# id = page_ids[[i]]
# url = paste0("https://www.wg-gesucht.de/", id, ".html" )
url = urls[[i]]
file = paste0(tempfile(), ".html")
# download
download.file(url, file)
# read
html = rvest::read_html(file)
urls
url_all_cities = "https://www.wg-gesucht.de/wohngemeinschaft.html"
html_all_cities = read_html(url_all_cities)
# get all the urls to the shared flats
a_tags = html_all_cities %>% html_elements("a")
a_tags_share_flats = a_tags[str_detect(tolower(html_text(a_tags, trim = T)), "share")]
links_locations_1 = html_attr(a_tags_share_flats, "href")
links_locations
main = links_locations[1:24]
rest = links_locations[25:length(links_locations)]
main
rest
str_subset(links_locations, "Wien")
links_localtions_2
links_locations_2
str_subset(links_locations_2, "Wien")
main = c(links_locations[1:24], str_subset(links_locations_2, "Wien"))
main
main = c(links_locations[1:24], str_subset(links_locations_2, "Wien"))
rest = links_locations_2[25:length(links_locations_2)]
main = c(links_locations[1:24], str_subset(links_locations_2, "Wien"))
rest = links_locations_2[25:length(links_locations_2)]
main = c(links_locations[1:24], str_subset(links_locations_2, "Wien"))
rest = links_locations_2[25:length(links_locations_2)]
# select 10 random links
samples_main = sample(main, 3)
samples_rest = sample(rest, 1)
sample = c(samples_main, samples_rest)
sample
# read the html from these locations
links = lapply(sample, function(u){
base = "https://www.wg-gesucht.de/"
url = glue("{base}{u}")
raw_html = read_html(url)
links = raw_html %>% html_elements("h3 > a") %>% lapply(function(x) {
href=html_attr(x, "href")
link=glue("{base}{href}")
}) %>% unlist
return(links)
}) %>% unlist
links
url
# download
download.file(url, file)
# download
a = download.file(url, file)
a
i = 1
print(i)
# id = page_ids[[i]]
# url = paste0("https://www.wg-gesucht.de/", id, ".html" )
url = urls[[i]]
file = paste0(tempfile(), ".html")
# download
tryCatch({
download.file(url, file)
# read
html = rvest::read_html(file)
size = getSize(html) %>% as.numeric()
price = getPrice(html) %>% as.numeric()
ab_bis = freiAb(html)
ab = ab_bis$ab
bis = ab_bis$bis
location = getLocation(html) %>% as.character()
wg = getWG(file)
print(paste0("WG: ", wg))
row = list(
price = price,
size = size,
location = location,
id = url,
wg = wg,
date = today,
ab = ab,
bis = bis
)
rows[[i]] = row
}, error = function(cond) {
next
})
url = "asdasd"
# download
tryCatch({
download.file(url, file)
# read
html = rvest::read_html(file)
size = getSize(html) %>% as.numeric()
price = getPrice(html) %>% as.numeric()
ab_bis = freiAb(html)
ab = ab_bis$ab
bis = ab_bis$bis
location = getLocation(html) %>% as.character()
wg = getWG(file)
print(paste0("WG: ", wg))
row = list(
price = price,
size = size,
location = location,
id = url,
wg = wg,
date = today,
ab = ab,
bis = bis
)
rows[[i]] = row
}, error = function(cond) {
next
})
url
# download
success = tryCatch({
download.file(url, file)
# read
html = rvest::read_html(file)
size = getSize(html) %>% as.numeric()
price = getPrice(html) %>% as.numeric()
ab_bis = freiAb(html)
ab = ab_bis$ab
bis = ab_bis$bis
location = getLocation(html) %>% as.character()
wg = getWG(file)
print(paste0("WG: ", wg))
row = list(
price = price,
size = size,
location = location,
id = url,
wg = wg,
date = today,
ab = ab,
bis = bis
)
rows[[i]] = row
return(TRUE)
}, error = function(cond) {
return(FALSE)
})
success
